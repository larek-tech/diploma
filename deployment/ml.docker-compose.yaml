networks:
  deployment_diploma-net:
    external: true


services:
  ml:
    container_name: ml
    build:
      context: ../ml
      dockerfile: Dockerfile
    ports:
      - "8888:8888"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: "python /project/src/server.py"
    env_file:
      - configs/ml.env
    environment:
      OLLAMA_BASE_URL: "${ML_OLLAMA_BASE_URL:-http://192.168.1.5:11434}"
      GIGA_CHAT_API_KEY: "${ML_GIGA_CHAT_API_KEY:-}"
      DEVICE: "${ML_DEVICE:-cuda}"
      RERANKER_MAX_LENGTH: "${ML_RERANKER_MAX_LENGTH:-2048}"
      ML_SERVICE_PORT: "${ML_ML_SERVICE_PORT:-8888}"
      DATA_SERVICE_HOST: "${ML_DATA_SERVICE_HOST:-192.168.1.5}"
      DATA_SERVICE_PORT: "${ML_DATA_SERVICE_PORT:-50051}"
      DEFAULT_RERANKER_NAME: "${ML_DEFAULT_RERANKER_NAME:-BAAI/bge-reranker-v2-m3}"
      HF_TOKEN: "${ML_HF_TOKEN:-}"
    deploy:
     resources:
       reservations:
         devices:
           - driver: nvidia
             count: 1
             capabilities: [ gpu ]
  exp:
    container_name: exp
    build:
      context: ../ml
      dockerfile: Dockerfile
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: "python /project/src/optuna_pipline.py"
    env_file:
      - configs/ml.env
    environment:
      OLLAMA_BASE_URL: "${ML_OLLAMA_BASE_URL:-http://192.168.1.5:11434}"
      GIGA_CHAT_API_KEY: "${ML_GIGA_CHAT_API_KEY:-}"
      DEVICE: "${ML_DEVICE:-cuda}"
      RERANKER_MAX_LENGTH: "${ML_RERANKER_MAX_LENGTH:-2048}"
      ML_SERVICE_PORT: "${ML_ML_SERVICE_PORT:-8888}"
      DATA_SERVICE_HOST: "${ML_DATA_SERVICE_HOST:-192.168.1.5}"
      DATA_SERVICE_PORT: "${ML_DATA_SERVICE_PORT:-50051}"
      DEFAULT_RERANKER_NAME: "${ML_DEFAULT_RERANKER_NAME:-BAAI/bge-reranker-v2-m3}"
      HF_TOKEN: "${ML_HF_TOKEN:-}"
    deploy:
     resources:
       reservations:
         devices:
           - driver: nvidia
             count: 1
             capabilities: [ gpu ]

